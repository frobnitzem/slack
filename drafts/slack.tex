\documentclass{sigplanconf}
%\documentclass[12pt]{article}

\usepackage[colorlinks=false]{hyperref}
\usepackage{amsmath,amssymb} 
\usepackage{fancyvrb}

%\newenvironment{template}{\sffamily}

\usepackage{graphics,epsfig}

\newcommand{\tens}[1]{\mathbf{#1}}

%\title{Network Tensor Contraction Engine: Project Demo Requirements Document}
%\author{David M. Rogers and James Horton}

%\title{ A High-Level Language for Basic Tensor Linear Algebra}
\begin{document}

\CopyrightYear{2016}
\setcopyright{acmcopyright}
\conferenceinfo{XSEDE16,}{July 17--21, 2016, Miami, FL, USA}
\isbn{978-1-4503-4755-6/16/07}\acmPrice{\$15.00}
\doi{http://dx.doi.org/10.1145/2949550.2949580}

% DOI
%\doi{10.475/123_4}

% ISBN
%\isbn{123-4567-24-567/08/06}

%Conference
%\conferenceinfo{PLDI '13}{June 16--19, 2013, Seattle, WA, USA}

%\acmPrice{\$15.00}



\title{ Efficient Primitives for Standard Tensor Linear Algebra}
\numberofauthors{1}
\author{
\alignauthor David M. Rogers\\
    \affaddr{University of South Florida}\\
    \affaddr{4202 E Fowler Avenue}\\
    \affaddr{Tampa, Florida 33620}\\
    \email{davidrogers@usf.edu}
}

\maketitle

\begin{abstract}
  This paper presents the design and implementation of low-level
library to compute general sums and products over multi-dimensional arrays (tensors).
Using only 3 low-level functions, the API at once generalizes
core BLAS1-3 as well as eliminates the need for most tensor transpositions.
Despite their relatively low operation count, we show that these transposition steps
can become performance limiting in typical use cases for BLAS on tensors.
The execution of the present API achieves peak performance on the same
order of magnitude as for vendor-optimized GEMM
by utilizing a code generator to output CUDA source code for all computational kernels.
The outline for these kernels is a multi-dimensional generalization
of the MAGMA BLAS matrix multiplication on GPUs.  Separate
transpositions steps can be skipped because every kernel allows arbitrary
multi-dimensional transpositions of the arguments.
The library, including its methodology and programming techniques, are
made available in SLACK.  Future improvements to the library include
a high-level interface to translate directly from a \LaTeX{}-like equation
syntax to a data-parallel computation.
\end{abstract}

\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10002950.10003714.10003715.10003719</concept_id>
  <concept_desc>Mathematics of computing~Computations on matrices</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010147.10010169.10010170.10010173</concept_id>
  <concept_desc>Computing methodologies~Vector / streaming algorithms</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10011007.10011006.10011041.10011047</concept_id>
  <concept_desc>Software and its engineering~Source code generation</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Mathematics of computing~Computations on matrices}
\ccsdesc[300]{Computing methodologies~Vector / streaming algorithms}
\ccsdesc[100]{Software and its engineering~Source code generation}

\printccsdesc

\keywords{tensor; blas; transposition; gpu}

\section{ Introduction and Background}

  Linear algebra on large arrays is central to scientific computing and all applied mathematical
modeling.  An increasingly large number of problem structures, however,
do not fit well to the simple vector-or-matrix model presented by basic linear
algebra subprograms (BLAS)\cite{blas} -- which form the currently accepted foundation for all
dense matrix operations.  Instead, most dense numerical algorithms fit much more naturally
to arrays -- a logical generalization including vectors (dimension 1) and matrices (dimension 2)
as special cases.  Their popularity can be easily seen by the thousands of packages based
on the general array implementation in python's {\em numpy} package.\cite{numpy}

  An $n$-dimensional array is defined as a block of data with $n$ indices, $i_0, \ldots, i_{n-1}$.
Each index has a contiguous range, $i_k = 0,\ldots,s_k-1$, where $s = \{s_0,\ldots,s_{n-1}\}$
is the shape of the array.  The set of all arrays of shape $s$ is denoted $\mathcal T_s$ (the terms
tensor and array are used interchangeably).
Here, we assume dense, non-symmetric arrays, which require memory space for
$\Pi_{k=0}^{n-1} s_k$ unique array elements.

  Despite their popularity, there are no general alternatives to BLAS operations for multidimensional
arrays.  At a minimum, these operations should allow addition, multiplication, contraction,
and transposition of arrays.  The {\em numpy} implementation also provides
several (map-like) per-element functions such as sine, cosine, and absolute values,
as well as (reduce-like) single-index operations such as min/max/sum.
Currently, {\em numpy}'s low-level implementation of these operations relies on a general
tensor transposition implemented on the CPU combined with a translation to BLAS.
It does not permit high-level abstraction or heterogeneous execution.

  Standard linear array computational kernels (SLACK) is intended to be a functional
replacement for BLAS that works efficiently with arrays.
Source code created in the course of the present work has been
made available in an open-souce library (USF-SLACK) that implements
this interface.  One major result of this work has been to demonstrate
that these kernels can be just as efficient as current high-performance
BLAS implementations.

  Several works have addressed the issue of generating code to efficiently
compute tensor operations in parallel, but fall short of a generally useful library.
%  As already noted, the MAGMA library
%uses a code generation strategy that shares .
%Our implementation is based on MAGMA's stencil for matrix multiplication
%on GPUs.\cite{}
The tensor contraction engine (TCE) translates
a domain-specific language for computational quantum chemistry
into a series of low-level FORTRAN loops for tensor
summations and transpositions.\cite{tce}  Much of the development effort for that
code has gone into identifying high-level reorganizations of the modeling equations,
and work is still underway to improve parallel execution methods.\cite{eapra15}
The cyclops tensor framework (CTF)\cite{ctf} is a similar project aimed at
implementing efficient tensor contractions for large tensors stored
in distributed global memory.  Its interface uses essentially equivalent {\tt contract} and {\tt sum}
functions, but specifies permutations by labeling all indices on each input tensor
using character strings and has non-standard
convention for choosing where results are output.

  One of the motivations for the present work is the observation stemming from these projects
that tensor transposition accounts for nearly half the execution
time of typical quantum chemistry problems.\cite{spaghetty}
The other is that a fast, standard library for contractions would greatly speed development
time for implementing many related methods.  As a case-in-point, there is already an 
$N^4$ scaling density-fitting method for carrying out CCSD
calculations which otherwise scale as $N^6$.\cite{ehohe12}
Unfortunately, that method has not yet been implemented because of the hundreds
of contraction steps that require intense effort to convert
down to simple, space-efficient BLAS operations.\cite{edepr14}

  Similar observations have motivated development on the tensor contraction generator (TCG),
\cite{tcg,wma10,wma11} and other low-level SIMD coprocessor generators
for tensor contractions.\cite{hshan15,tnels15}
Unfortunately, many of these codes are not generally available.
Performance results for TCG were only reported for double precision
(shown to achieve a peak of around 40 Gflops), but not compared to vendor-optimized GPU
BLAS kernels.  In addition, the lack of a standard API or
straightforward syntax for specifying contractions has can make a project unaccessible.
Redesigning the syntax of these operations both eliminates
transposition cost as well as divorces the algorithm from its domain-specific context.

  A notable contribution was made by the general-purpose tensor
operation library, TAL-SH.  It is based on carefully optimized
tensor transposition routines for popular parallel hardware,\cite{dlyak15}
and the source has become available since the original draft of
this work was posted.

  A few researchers have considered simply finding optimal methods for
translating of tensor contractions to standard BLAS calls.\cite{slice,enapo14,epeis15}
At a best, these translations still incur the
cost of added tensor transpositions.  In the worst case, the translation requires multiple
level-1 BLAS operations with sparse data accesses that cannot be made cache-efficient
and thereby slow execution times.

  As an alternative strategy, the present work implements
tensor contraction directly without the need for transposition.
The contraction routines follow the general strategy pioneered by
the %tunable matrix multiplication kernel in the
Matrix algebra on
GPU and multicore architectures (MAGMA) library,\cite{magma}
but adds awareness for multi-index input and output tensors.  The result
is an identical computation for each tile of the output matrix, using
modified read and write loops for accessing those tile locations.

  Section~\ref{s:low} of this paper presents a low-level API comprised of only three basic operations,
-- {\tt tscale}, {\tt tadd}, and {\tt tdot}.  These  cover all basic addition, multiplication, transposition,
and contraction operations (Table~\ref{t:blas}).
They are therefore a generalization of the core of BLAS levels 1-3
-- excluding routines involving packed storage, symmetry, $L_1$ norms, or specialized rotation functions.
Section~\ref{s:gen} outlines the operation of the code generator which
achieves high performance kernels by automatically generating implementation
code for arbitrary thread block and output tile sizes.
%Section~\ref{s:high} presents the high-level organization used to automatically execute a
%set of tensor algebra operations in parallel, including automatically
%managing intermediate values.

  A performance study is presented in Sec.~\ref{s:perf}.  Using the same tile
sizes as MAGMA without separate optimization
or using texture mapping, the kernels corresponding to SGEMM (single precision
general matrix multiply) are about 71\%
of the speed of the performance-tuned MAGMA library.
Significantly, these speeds are maintained
for multi-index tensor products, where they hold a significant advantage over a na\"{\i}ve
BLAS-based implementation strategy.
It is shown that emulating those tensor products by a
(faster) SGEMM sandwiched between tensor transpositions
decreases overall performance by up to a factor of 4.

\section{ Low-level Contraction Operations}\label{s:low}

  The two routines, {\tt tadd} and {\tt tdot}, cover all addition, multiplication, transposition,
and contraction operations.  The additional routine, {\tt tscale}, is the equivalent of BLAS' \_SCAL,
and is included for completeness.
The routine {\tt tadd} is a fused tensor transpose plus add.
Its inputs are two tensors, $A \in \mathcal T_{s_0,s_1,\ldots,s_{n-1}}$
and $B \in \mathcal T_{t_0,t_1,\ldots,t_{n-1}}$, of the same dimensionality ($n$),
along with an index permutation, $\sigma$, mapping uniquely from indices of $B$ to indices of $A$.
The two tensors must have equivalent shapes under permutation, so $t_k = s_{\sigma(k)}$.

  For compact notation, we define multi-indices as a complete tuple of indices,
$I = (i_0, i_1, \ldots, i_{n-1})$, $i_k \in \{0,1,\ldots,s_k-1\}$, and permutations of tuples as
$I = \sigma J$, where $i_{\sigma(k)} = j_k$.
For numerical efficiency, two scaling factors are included in {\tt tadd}
inputs, and the result of the operation is stored in-place in the tensor $A$.
The combined operation is logically,
\begin{equation}
\text{\tt tadd} \quad A[I] := \alpha A[I] + \beta B[\sigma^{-1} I]
\end{equation}

  The use of a permutation on the indices of $B$ is necessary to remove the need
for transpositions of input arguments.  The input matrix, $A$ is not transposed.
Its index order carries through to the output.  Necessary transposes can usually
be folded into the eventual function consuming the output of `{\tt tdot}'.  Nevertheless,
a matrix transpose as expressed by {\em numpy}'s {\tt transpose}($A, \sigma^{-1}$),
can be written as {\tt tadd}(0.0, Zero, 1.0, $A$, $A$.shape, $\sigma$).

%  This obviously generalizes {\tt axpy}, which itself generalizes {\tt copy} and {\tt scal}.
%In addition, changing the binary operation is comparatively much simpler from the end-user's
%perspective when code is generated from a template.  This allows, for example,
%building up direct products.

  Finally, the structure of `{\tt tadd}' can be relaxed to allow both reductions along
several indices of $B$ or introduction of new indices into $B$.  Both can be understood
as broadcasting operations\cite{numpy} involving an index of size $1$ from one
array and an index with an arbitrary size from the other.  Note that inserting an index of size 1
is only a logical operation, and does not change the memory layout of the array.

  Formally, we relax the restriction that $t_k = s_{\sigma(k)}$ to either
$t_k = s_{\sigma(k)}$ or else one of: $t_k = 1$ or $s_{\sigma(k)} = 1$.
The permutation still maps uniquely from indices of $B$ to indices of $A$.
All memory accesses to $A$ and $B$ are computed via strides.
The stride is simply set to zero for any index with size 1.
Explicitly, $A[I]$ is read from the memory location,
\begin{align}
A &+ \sum_{k=0}^{n-1} i_k \text{stride}_k \\
\intertext{where}
\text{stride}_k &= \begin{cases}
0, & s_k = 1 \\
\Pi_{w=0}^{k-1} s_w, & \text{o.w.}
\end{cases}
\end{align}

  In the non-broadcast case, the stride for dimensions with size 1 is irrelevant,
since the only allowed index is zero.  In the broadcast case, the stride of zero
ensures that all indices into that dimension appear as copies of the zero-index.
For a reduction, such as $A := A + \beta \sum_j B_j$,
an index is inserted into $A$ with size 1 and stride 0.
Summation along $j$ will accumulate into to the same output location.
For a broadcast, such as $A_{ij} = \alpha A_{ij} + \beta B_j$, a new first
index is added to $B$ with size 1 and stride 0.

  Many other useful functions also follow the same basic pattern of {\tt tadd},
replacing `+' with any commutative operation on 2-arguments.
The direct product just replaces `+' with, `*'.  Reductions can usefully replace
`+' with `min' or `max', etc., but require atomic updates when each thread makes its
final output.

\subsection{ Tensor Contraction}

  The `{\tt tdot}' operation expresses a general tensor contraction.  Tensor
contractions are a special type of marginalization product function\cite{mpf} where
indices in $A \in \mathcal T_{s_0,\ldots,s_{n_A-1}}$ (and respectively $B_{t_0,\ldots,t_{n_B-1}}$)
are divided into two disjoint sets: $n$ ``inner'' and $n_A - n$ (resp. $n_B-n$) ``outer'' indices.
The inner indices are to be summed over and must have sizes that
match between $A$ and $B$.  The outer indices are not constrained,
and are output to a third tensor, $C$.  This output tensor therefore has dimension
$n_C = n_A + n_B - 2 n$.

\begin{figure}
\includegraphics[width=0.45\textwidth]{contr.eps}
\caption{Role of index permutations in the tensor contraction.
Permutations of $A$ and $B$ indices specify both the order of the
contraction result, $C$, and an artificial ordering on the contracted indices.
For comparison, a common emulation using GEMM sandwiched between tensor
transpositions is shown below.}\label{f:contr}
\end{figure}

  We specify a generic tensor contraction using two permutations,
$\sigma_A$ and $\sigma_B$, mapping indices in A (resp. B), to indices in
an expanded, logical tensor, $C' \in \mathcal T_{u_0, \ldots, u_{n_C+n-1}}$.
The complete tensor contraction is a sum over the last $n$ indices of this expanded
tensor,
\begin{align}
\text{\tt{tdot}} \quad C[u_0,\ldots,u_{n_C-1}] &:= \alpha \sum_{u_{n_C}, \cdots, u_{n_C+n-1}} C'[u_0,\ldots,u_{n_C+n-1}]  \notag \\
 & + \beta C[u_0,\ldots,u_{n_C-1}] \\
 C'[u_0,\ldots,u_{n_C+n-1}] &= A[i_0,\ldots,i_{n_A-1}] B[j_0,\ldots,j_{n_B-1}]
\end{align}

  The expanded tensor is illustrated in Fig.~\ref{f:contr}.
The indices $\{u\}$ are such that $u_{\sigma_A(k)} = i_k$ and $u_{\sigma_B(k)} = j_k$.
The outer indices of $A$ ($B$) thus map uniquely to the left part of $C'$.
The index, $k$, of tensor $A$ (resp. $l$ of $B$) is outer if
$\sigma_A(k) < n_C$ ($\sigma_B(l) < n_C$), and contraction enforces $\sigma_A(k) \ne \sigma_B(l)$.
The inner indices go to the right in pairs, so $k'$ is an inner index of $A$ (resp. $l'$ of $B$)
if $\sigma_A(k') \ge n_C$ ($\sigma_B(l') \ge n_C$).
Contraction enforces exactly one equivalent map, $m \iff (k',l')$, where
$u_{m} = \sigma_A(k') = \sigma_B(l')$, for each $m \ge n_C$.

  The {\tt tdot} operation encompasses a large subset of BLAS 1-3.  Table~\ref{t:blas} lists several
translations.
\begin{table*}
\begin{tabular}{lll}
$x \gets \alpha x$ & \_SCAL(N,$\alpha$,X) %\\
    & tscale($\alpha$, X, N) \\
$y \gets x$ & \_COPY(N,X,Y) %\\
    & tadd(0.0, Y, (N), 1.0, X, (N), (0)) \\
$y \gets \alpha x + y$ & \_AXPY(N,$\alpha$,X,Y) %\\
    & tadd(1.0, Y, (N), $\alpha$, X, (N), (0)) \\
r $\gets x^T y$ & \_DOT(N,X,Y) %\\
    & tdot(0.0, R, 1, 1.0, X, (1,N), (0,1), Y, (N), (1)) \\
$y \gets \alpha A x + \beta y$ & \_GEMV(`T', M, N, $\alpha$, A, X, $\beta$, Y) %\\
    & tdot($\beta$, Y, 1, $\alpha$, A, (N, M), (0,1), X, (M), (1)) \\
$y \gets \alpha A^T x + \beta y$ & \_GEMV(`N', M, N, $\alpha$, A, X, $\beta$, Y) %\\
    & tdot($\beta$, Y, 1, $\alpha$, A, (N, M), (1,0), X, (N), (1)) \\
$A \gets \alpha x y^T + A$ & \_GER(M, N, $\alpha$, Y, X, A) %\\
    & tdot(1.0, A, 2, $\alpha$, X, (N, 1), (0,2), Y, (M, 1), (1,2)) \\
$C \gets \alpha A B + \beta C$ & \_GEMM(`N', `N', M, N, K, $\alpha$, B, A, $\beta$, C) %\\
    & tdot($\beta$, C, 2, $\alpha$, A, (N,K), (0,2), B, (K,M), (2,1)) \\
$C \gets \alpha A^T B + \beta C$ & \_GEMM(`N', `T', M, N, K, $\alpha$, B, A, $\beta$, C) %\\
    & tdot($\beta$, C, 2, $\alpha$, A, (K,N), (2,0), B, (K,M), (2,1)) \\
$C \gets \alpha A B^T + \beta C$ & \_GEMM(`T', `N', M, N, K, $\alpha$, B, A, $\beta$, C) %\\
    & tdot($\beta$, C, 2, $\alpha$, A, (N,K), (0,2), B, (M,K), (1,2)) \\
$C \gets \alpha A^T B^T + \beta C$ & \_GEMM(`T', `T', M, N, K, $\alpha$, B, A, $\beta$, C) %\\
    & tdot($\beta$, C, 2, $\alpha$, A, (K,N), (2,0), B, (M,K), (1,2)) \\
\end{tabular}
\caption{Translation from C, row-major shorthand notation to legacy BLAS and
to {\tt tadd} / {\tt tdot} for arrays of dimension 1 and 2.  All lower dimensions (IDX, etc.) are 1.
Argument order for {\tt tadd} is $\alpha$, A, A shape, B, B shape, B permutation.
Argument order for {\tt tdot} is  $\beta$, C, dimension of C, $\alpha$, A, A shape, A permutation, B, B shape, B permutation.  Note that {\tt tdot} is symmetric to interchanging $A$ and $B$.}\label{t:blas}
\end{table*}

  A straightforward implementation of tensor contraction is provided
by {\tt tensordot} in the {\em numpy} library
with a fixed ordering of all outer indices.\cite{numpy}
First the tensors $A$ and $B$ are transposed to $A'$, $B'$,
so that the inner indices of both are moved to the right-hand side in matching order.
The outer indices are left in their input ordering.  Because they are contiguous, all outer indices
can now be addressed using a single, combined index, and similarly for the inner indices.
The tensor product then reduces to a matrix product, $A' B'^T$.
To emulate the present definition of contraction, this end result needs to be supplemented
by a final transpose to re-order the output indices.  The efficiency of this method is compared to the
direct kernel versions in Sec.~\ref{s:perf}.

\section{ Code Generation Strategy}\label{s:gen}

  The low-level kernel execution for {\tt tdot} is essentially a GEMM, but
has the added difficulty of dealing with arbitrary numbers of indices.
The most simplistic implementation is to combine all $n_C$ output indices
of $C$ into one omnibus dimension, and all $n$ inner indices
into another.  Each iteration of a double-loop decodes the corresponding
locations in $A$, $B$, and $C$.  The timings for this implementation, using
1 CUDA thread per output location peak around 10 Gflops and are in most cases
worse than the transpose-sandwiched version.

  A much better performance is achieved by adopting the MAGMA library
strategy of loop nesting optimized for the Fermi architecture.\cite{magma,rnath10}  There, a $T_A \times T_B$ block of threads each
compute $W_A \times W_B$ output locations to produce an $T_A W_A \times T_B W_B$
output tile of $C$ for each thread block.  Each thread sums over the whole contracted
dimension, $K$.  The outline of the procedure is as follows:\cite{rnath10}
\begin{enumerate}
\item Each of the $T_A \times T_B$ block of threads allocates and zeros
a $W_A \times W_B$ accumulator, {\tt acc} in thread-local, register space.
\item An outer loop over contracted indices works on $N$ additions into {\tt acc} at a time:
  \begin{enumerate}
    \item All threads work together to pre-fetch a $T_A W_A \times N$ tile of $A$ and a
    $T_B W_B \times N$ tile of $B$ into shared memory.
    \footnote{  An additional optimization done by MAGMA and the present work
is to have each thread pre-fetch the next tile of $A$ and $B$ as early as possible
into thread-local register space.  Then the shared memory is filled from those registers
rather than directly from global device memory.  In practice, the next copy starts
just before each accumulation step.  The assignment of parts of $A$ and $B$
to pre-fetch is unrelated to the thread's output location.}
    Synchronization is required around this step.
    \item Threads work individually to accumulate $N$ additions into {\tt acc} using GER-like operations.
  \end{enumerate}
  \item Each thread writes {\tt acc} to the appropriate output locations in $C$.
\end{enumerate}

  This strategy decreases the cost of memory movement by relying on the
hardware to combine reads from all threads into sequential
accesses of $A$ and $B$ from CUDA global memory.
Both copies from $A$ and $B$ as well
as computation of the output tile are parallelized among the thread block.

  We have adapted MAGMA's tile-based strategy to {\tt tdot} by defining the output tile
to be composed of 2 omnibus dimensions -- all output indices from $A$
and all output indices from $B$.
%  The thread and work blocks are therefore sub-divided
%further for each index with strides, $T_A = \{T^A_k | \sigma_A(k) < n_C\}$,
%and similarly for $T_B$, $W_A$, and $W_B$.
The procedure outline is the same, but access to tiles of $A$, $B$, and $C$
is strided by mapping the single thread index to the tensor multi-index.
The code generator works for fixed tensor dimensions and permutations, so that
the index decoding steps are explicit instead of loops over arbitrary number of dimensions.
Copies in the nested inner loops have known sizes, and are written out explicitly
without requiring integer division operations.

  Although the output tile is logically indexed by output dimensions of
$A$ and then $B$ (as for the output of {\tt tensordot} -- Fig.~\ref{f:contr}),
the actual output is done directly to the correct locations in $C$.
%This only requires putting the strided access in the right place.
It has a large practical advantage in that the physical
memory is only accessed once.

  We experimented with multiple strategies for organizing kernels.
In the naming convention we adopted, each kernel name has 3 parts,
\begin{itemize}
\item The tile size of the logical output (both inner and outer indices) from the whole thread block -- $n_C + n$ integers.
\item The number of threads working on each output index -- $n_C$ integers, each dividing the output size.
\item The permutations, $\sigma_A$ and $\sigma_B$.
\end{itemize}
The cache size for the entire thread block can be computed from the
logical output tile size, $s$, as
\begin{align}
&\prod_{k=0}^{n_A-1} s_{\sigma_A(k)} + \prod_{k=0}^{n_B-1} s_{\sigma_B(k)} \\
\intertext{for prefetching,}
&+ \prod_{k=0}^{n_C-1} s_k \\
\intertext{for accumulation, and}
&+ \prod_{\{k | \sigma_A(k) < n_C\}} s_{\sigma_A(k)}
 + \prod_{\{k | \sigma_B(k) < n_C\}} s_{\sigma_B(k)}
\end{align}
for optimized copying from shared memory during the accumulation loop.


  This naming scheme is a compromise that simplifies the usage of the kernel,
but does not specify the division of prefetches among processors.
Even for a 2-index contraction of 4-dimensional tensors like that in Fig.~\ref{f:contr},
there are 6 logical output dimensions and 4 dimensions to choose
the thread block size.  Because the search space is large, the kernel parameters
tested were based on MAGMA tile sizes or chosen heuristically by picking
larger tile sizes for the most memory-contiguous dimensions.

\section{ Performance Results}\label{s:perf}

  This section presents timing results for single-precision {\tt tdot} operations computed
on an NVIDIA GTX980 (1.38 GHz, 4 GB memory) with NVIDIA proprietary
kernel driver version 346.72 for Ubuntu server 14.10 compiled with nvcc 7.0.27.
Reported timings are the fastest out of 10 trials and do not include CPU to GPU transfer times.
This GPU is one of the first releases to use the new Maxwell redesign of
the K20.  Because it has been designed for consumer graphics,
there are fewer double-precision floating point units per stream processor.
MAGMA timings for DGEMM (double precision matrix multiply)
on this GPU peaked around 150 Gflops, while those
for SGEMM were significantly higher at 4000 Gflops.  Because of this limitation,
only single precision timings are reported below.

  The GM204 chip used by the GTX980 schedules thread blocks nondeterministically
to one of 16 streaming multiprocessors (SMs).  All blocks scheduled to a given SM
utilize that SM's 96 kB block of shared memory and its
64 kB L1/texture cache for holding register values. 
Differing from the Kepler architecture, the register and shared memory spaces are separate.
 Within the SM, each thread
block is further divided into warps of 32 threads each.
Warps assigned to a scheduler can be either running an instruction in parallel
(1 CUDA core per thread) or waiting for memory access.
On the GM204, there are 4 warp schedulers per SM.

  For an example, suppose a standard matrix multiplication is scheduled
using a thread blocks containing 96 threads each.  Those blocks would be
divided into 3 warps.  If an SM undertook to execute 2 thread blocks at a time,
it would be managing 6 warps - each of which would alternate between prefetching
elements and doing additions from shared memory to register space
before finally outputting to tiles of $C$.

  Fig.~\ref{f:gflops} compares alternative methods for computing the 4-index
tensor contraction illustrated in Fig.~\ref{f:contr}.  The horizontal
axis shows $N$, the size of each dimension for the case where all shapes are
$\{N,N,N,N\}$ hypercubes.
The upper panel shows throughput for the SGEMM kernel on matrices of size $N^2 \times N^2$.
The right side of Fig.~\ref{f:contr} shows how the central step
of the contraction is essentially a matrix multiplication.  The test kernel
computes one output element of the matrix per thread, and has fairly
constant performance around 10 Gflops.
The optimized MAGMA 1.6.1 kernel and cuBLAS performance is significantly
higher, peaking around 4500-5000 Gflops (very near the theoretical peak of
5234).  MAGMA uses a 16$\times$16 thread
block to compute 36 output elements each -- for an output tile size of 96$\times$96.
cuBLAS shows variable performance, likely the result of a larger tile size.

  Notably, Fig.~\ref{f:gflops} shows that {\tt tdot} achieves performance on the same order
of magnitude as state of the art optimized BLAS routines.
Using the same tile sizes as MAGMA (16$\times$16 thread block and 96$\times$96 output tile)
for the generated {\tt tdot} kernel gives around 2500 Gflops compared to MAGMA's 3500.
Because the basic outline of the kernel is identical between MAGMA and
{\tt tdot}, the lower performance may be due to
minor differences in the layout and orderings of copies between global memory,
shared memory, and registers.

\begin{figure}
{ \centering
\includegraphics[width=0.45\textwidth]{sgemm.eps}
\includegraphics[width=0.45\textwidth]{gflops.eps} }
\caption{Performance comparison between MAGMA and generated contraction
kernel, {\tt tdot}.  The upper panel compares the fastest versions of the SGEMM kernel,
which happened to be NN for MAGMA and NT for {\tt tdot}.  The lower panel
shows that including transposes greatly decreases overall throughput for emulating {\tt tdot},
while a direct 4-index {\tt tdot} kernel performs at about half the speed of {\tt tdot} for SGEMM.
The designations NN and NT refer to the index ordering of the input matrices,
as in Table~\ref{t:blas}.}\label{f:gflops}
\end{figure}

  The lower panel of Fig.~\ref{f:gflops} compares the overall performance for the full
4-index contraction of Fig.~\ref{f:contr} using a {\tt tdot} kernel created
to directly compute the contraction against an emulation strategy using
SGEMM sandwiched between transposes.  The emulated timings are computed
by adding the timings for transposing both inputs, performing SGEMM using the MAGMA library,
and transposing the final output.  Even though the matrix multiplication
step scales as $O(N^6)$, while transposition scales as $O(N^4)$,
the matrix transpositions severely limit the overall throughput.  It should be noted
that tensor sizes larger than N = 128 could not be tested as they exceed the 4GB memory
limit of the device.

  Oscillations in throughput in Fig.~\ref{f:gflops}
are due to many cases tested where the tile size does
not divide the tensor size.
For high-order tensors, these become the common case.
Because padding space grows exponentially in the number
of dimensions, padding is not feasible without
resorting to the transpose strategy.
Comparison between CUBLAS and MAGMA is not direct,
since MAGMA was not specifically tuned for the current,
Maxwell architecture.

\begin{figure}
{ \centering
\includegraphics[width=0.45\textwidth]{cgemm.eps}
\caption{Performance of an $N\times N$ complex matrix multiplication
with CUBLAS/MAGMA
compared against an $[N\times N\times 4] \cdot [N\times N\times 2]$
contraction with {\tt tdot}.  In this comparison, the number of floating point operations
is the same, but the contraction requires twice as many reads.
The difference from the top panel of Fig.~\ref{f:gflops} clearly illustrates
the impact of memory accesses.}\label{f:cplx}}
\end{figure}

  To test a tensor contraction case with some low-dimensional indices,
Fig.~\ref{f:cplx} presents performance results for the 2-index
contraction,
\begin{equation}
C_{ij\alpha} = \sum_{k\beta} A_{ik\alpha\beta} B_{kj\beta}
,
\end{equation}
where indices $\alpha,\beta \in \{0,1\}$ are only size two.
The variable $N$ labels the size of the $i$ (and also the $j, k$)
dimension.
This is the matrix representation of complex multiplication
when
\begin{equation}
A_{ik} = \begin{bmatrix}a_{ik}, -a_{ik}'\\
a_{ik}', a_{ik}\end{bmatrix} .
\end{equation}
The tile-size used was identical to the MAGMA kernel,
which seemed to give the best performance.
Despite requiring twice as many loads from $A$,
the performance of {\tt tdot} is of the same
order of magnitude as 
straightforward complex matrix multiplication.
This also illustrates a case where where
tensors can play an important role, since
higher-dimensional Clifford algebras (e.g. quaternions) share the same
contraction structure, but with more $\alpha$ indices.

  Fig.~\ref{f:pct} provides further detail on the time spent during tensor transposition
{\em vs} computing matrix multiplication.  The transpositions occupy a significant
fraction of time for all sizes.  In fact, for most reasonable tensor sizes, the transposes
occupy more than 60\% of the time.  The requirement for separate transpositions
explains the low overall throughput for emulating a direct tensor contraction.

\begin{figure}
{ \centering
\includegraphics[width=0.45\textwidth]{pct.eps} }
\caption{Breakdown of time spent in emulated {\tt tdot} for Fig.~\ref{f:contr}.
Host $\leftrightarrow$ device memory transfer time is not included.
SGEMM timings are copied from MAGMA's timings on an $N^2 \times N^2$ matrix.
Transpose timings (performed on a 2.5 GHz Intel Core i7 processor with 1600 MHz DDR3 RAM)
count only the transposes that would be done during {\tt tensordot}
within the {\em numpy} library.
Even for large tensors, a majority of the time is spent in CPU transpose.}\label{f:pct}
\end{figure}
  
\section{ Conclusions and Future Work}

  We have shown that computing tensor transpositions can be
a significant
bottleneck to general tensor algebra, both in terms of performance
and memory requirements.  Legacy BLAS routines do not provide
a means to cope with this issue.  Instead, we propose a simple and
general API for combining transpositions directly into
tensor contractions and summations.
This provides a means to hide the cost of tensor transpositions
inside the data access already performed during the
normal course of tensor multiplication.
We have shown a preliminary implementation of this API that provides
speed comparable to high-performance BLAS.

  Many factors influence the efficiency of {\tt tdot} kernels.
These include both the shape of the tensors $A$, $B$, and $C$
relative to the thread and work block shape as well as the
index permutations applied to $A$ and $B$.
In all cases, it is reasonable to expect the largest
performance improvements
when $A$, $B$, and $C$ are all accessed sequentially within each tile.

  Tensors present unique challenges in that high-dimensional tensors
have many indices with small shapes.
%The performance data in Fig.~\ref{f:gflops}
%shows non-uniform behavior partly because each dimension has to be expanded
%to the nearest output tile-size.
These make padding unfeasible in a tensor setting,
since the size required grows exponentially
with the tensor dimensions.
However, if a few small dimensions appear in the minor (sequential)
indices of $A$, $B$, or $C$, then a custom kernel
can be written to access these inputs sequentially.
The ordering of the major indices of the
tensors becomes less important as tensor sizes grow.

  As expressed by Ref.~\cite{magma}, cases with a few small
dimensions are precisely where code generation
is most useful.
They found large numbers of kernels with smaller tile sizes
that performed nearly as well as those with larger tiles
because of their ability to combine sequential access patterns
with loop nesting working cache-sized-at-a-time.

  Future work should continue to investigate more efficient
ways of generating and choosing templates, as well as
more high-level, science-friendly interfaces to linear operations.
We did not have the opportunity to investigate methods to
optimize host to device memory transfer -- which could accomplish a
partial transposition, or to explore non-square matrices.
Developing an accurate cost model for device
performance would help by allowing a fast scan of the kernel
parameter search space.
Fewer errors and greater reproducibility can also be
gained by providing a high-level language to compose
chains of {\tt tdot} and {\tt tadd}
operations on multiple input and output tensors.
A separate paper describes the SLACK library's implementation of a
DAG scheduler for tensor operations
based on a \LaTeX{} equation input syntax.

\section{ Acknowledgments}
  This work was supported by the USF Research Foundation,
NSF MRI CHE-1531590, and NSF XSede Resource Allocation TG-ASC130043.
We thank Jeff Hammond for lively discussions leading to this work.

\section{ Software}
  Reference and optimized implementations of the SLACK project
are made available via
(\href{https://github.com/frobnitzem/slack}{github.com/frobnitzem/slack}).
All code will be released under terms of the GNU GPL.

\bibliographystyle{abbrv}
\bibliography{lit}

%\balancecolumns
\end{document}


\section{ Introduction}

  Nevertheless, the high-level organization
chosen for the present project is a departure from the organization of the high-level
routines in the MAGMA library.  Here, we have chosen to use a strict functional
syntax tree, as opposed to that library's use of a series of imperative routines written
using high-level aliases for tuned low-level codes.  The imperative style allows greater
control over the synchronization and memory management during
intermediate steps in the computation.  In principle this allows hand-tuning to
achieve the highest possible performance.  The functional programming style
adopted here acknowledges that in practice these low-level details are very
difficult to anticipate, and that it is impossibly time-consuming to hand-tune every
high-level function.  Instead, SLACK expresses all compound operations using an abstract
syntax tree, which is executed using a generic parallel scheduling back-end.
Optimizations are expected to be implemented through syntax
tree transformations.

  The Theano project\cite{theano} has a similar perspective.  It builds a directed acyclic graph
representation of a series of tensor operations in order to gain run-time-efficiency
by simplification on that representation.  Another noteworthy python
library using functional representations to generate code is the FEniCS project.\cite{fenics}
That project implements a domain-specific language for partial differential
equations on finite element meshes from which C++ code is generated.

  The tensor contraction engine (TCE) translates
a domain-specific language for tensor contractions occurring in the field of
chemistry into a series of matrix BLAS operations and low-level code for transpositions.\cite{tce}
It also produces the equations in \LaTeX{} form as a by-product.
Distinct from this work, TCE, along with a related engine, \cite{ctf}, make use of
statically planned distributed-memory array storage and execution.
One of the motivations for the present work is the observation that tensor transposition accounts for
nearly half the execution time of typical problems.  Redesigning the high-level
syntax of these operations both eliminates this additional cost as well as
divorces the algorithm from its domain-specific context.

\section{ Functional Interface}\label{s:high}

  The SLACK library expresses a series of tensor operations using a
directed acyclic graph (DAG).  Nodes in the DAG correspond
to intermediate results, while arrows show data dependencies.  The leaves
of the tree are input tensors, and the root represents
the final output result.  Internally, each DAG node is represented as an
operation and a list of dependencies.

\begin{figure}
{\centering
\includegraphics[width=0.45\textwidth]{Cstart.eps}
\includegraphics[width=0.45\textwidth]{C.eps}}
\caption{Initial (left) and simplified (right) expression trees parsed from \LaTeX{} source for Eq.~\ref{e:C}.  Execution order is from bottom to top, with the topmost node representing the output, $C$.  Index symbols have been converted to permutations, $\sigma$, shown near the end of each arrow. Contractions are labeled with
$* \alpha$ and the dimension of the output tensor in brackets.  Scales ($\alpha$ / $\beta$) are shown at the tops of arrows where applicable.  The topmost 4-index contraction forms
the example shown in Fig.~\ref{f:contr}.}
\label{f:dag}
\end{figure}

  Figure~\ref{f:dag} shows the initial and final DAGs resulting from
parsing the mathematical expression,
\begin{equation}
C^{ab}_{ij} = -\sum_{kc} \left(
                        \sum_Q \text{Uoo}^Q_{ki} \text{Uvv}^Q_{ac}
                        - 0.5 \sum_{ld} V_{kdlc} t^{ad}_{li}
                                     \right) t^{bc}_{kj}  \label{e:C}
\end{equation}
During (bottom-up) parsing, permutations are generated by maintaining an ordered list of `active' indices
(as well as scaling factors) at each stage during the parse.  At the end of the parse, a final tensor transpose
is added to convert these active indices to the desired output order.
Superscripts are put before subscripts.
So, for example, $\sum_Q \text{Uoo}^Q_{ki} \text{Uvv}^Q_{ac}$
has order $[kiac]$, and its contraction with $t^{bc}_{kj}$ originally had the order, $[iabj]$.
The final result (right of Fig.~\ref{f:dag}) is obtained by a top-down
simplification that pushes scaling operations and transpositions down to the leaves.

  Given an appropriate performance model, the DAG could be the target of more
advanced optimization techniques.  Opportunities for optimizations include re-ordering
indices at intermediate stages to maximize the efficiency of {\tt TDOT},
statically assigning the memory locations for intermediate values, or duplicating
the computation of some intermediates to lower the amount of communication
or synchronization overhead.

  The actual execution of the DAG involves one further step.
Each intermediate value, represented by a node, requires physical memory for storage
during execution.  Both the {\tt tbin} and {\tt tdot} operations are defined so as
to output to one of their input values.  That input has to be copied whenever it
has more than one parent.  In the tree shown in Fig.~\ref{f:dag}, every node has exactly
one parent, and so there are only 3 allocations required -- essentially the nodes marked with `Zero'.
Note that the third execution step will combine the lower two contractions and release
the memory allocated to hold its $A$ value.  Caching recently free-d memory regions would allow
this space to be immediately re-used for the last allocation.

  The full SLACK allows building compound expression using shared,
named intermediate values.  Those named intermediates can have more
than one parent, and are automatically free-d when no longer referenced.
Execution uses a two-pass strategy.  The first pass creates metadata for each
node -- the reference count and a pointer to the actual tensor location.
The second pass uses the metadata to dynamically decide
memory locations at execution time.
  An experimental adaptation to utilize the dynamic memory management of the
StarPU library is underway.

\section{ Motivation}

  Electronic structure for the water molecule (10 electrons) can be solved at high accuracy
in under 20 seconds or so.  However, the calculations scale in
compute time and mostly storage space like $N^4$ (cheap) or $N^6$ (great) or
even higher -- where $N$ is the number of electrons.  Very large
calculations run 100s of waters:
\href{http://www.nwchem-sw.org/index.php/Benchmarks}{NWChem Benchmarks}
And can complete in around 200 seconds on thousands of processors.

  The cited software (nwchem) utilizes a code generator (named the tensor contraction engine)
for carrying out matrix manipulations needed to solve the electronic structure problem.
This engine has greatly extended the allowable problem size, but has met a design limit
in its inability to distribute different tensor contractions among compute nodes.
However, this is exactly the structure required by a new $N^4$ scaling
density-fitting method for carrying out CCSD calculations which otherwise scale
as $N^6$.   The science is done, but the numerics is lagging. 

\section{ Objectives}

  The challenge is to generate codes to do communication
and high-level ``contraction'' operations that are specialized
to a given series of tensor operations.
The problem is therefore divided into two parts.
The first part is translating from a high-level mathematical language into an
intermediate sequence of {\tt contract} operations,
as well as interfacing those operations with quantum codes to generate
the initial tensors and to handle the results.

  The second part, the focus of this document, is to provide a system
for executing the tensor contractions.  Because of the large sizes of
data and number of operations involved, this will require a robust system
for tracking data locations and remaining tasks.  The execution system
will use this information to organize data transmission in-between carrying
out individual operations using math acceleration libraries.

  The project will be successfully concluded when a program has been
generated that carries out a series of contractions as specified by a general
dependency DAG.  This will be applied to compute the electronic structure of a
small molecule.  Future work will focus on optimizing the individual operations
as well as the execution order to profile and improve memory performance.

\section{ Test Problem}

\subsection{ Reference Problem Translation}

  The DAG of primitive tensor operations can be read directly
from a subset of the \LaTeX{} math syntax and/or built by automated methods.
For example, the \LaTeX{} source for the following equations:
\begin{align}
R_{ij}^{ab} &= \sum_Q M_{ai}^Q M_{bj}^Q + A_{ij}^{ab} + B_{ij}^{ab} + U_{ij}^{ab} + U_{ji}^{ba} \\
A_{ij}^{ab} &= \sum_{cd} t_{ij}^{cd} \sum_Q M_{ac}^Q M_{bd}^Q \\
B_{ij}^{ab} &= \sum_{kl} t_{kl}^{ab} \left( \sum_Q M_{ki}^Q M_{lj}^Q
                 + \sum_{cd} t_{ij}^{cd} \sum_Q M_{kc}^Q M_{ld}^Q \right)
\end{align}
was parsed into the expression tree in Fig.~\ref{f:dag}.

\begin{figure*}
\includegraphics[width=0.9\textwidth]{dag.eps}
\caption{Initial expression tree parsed from \LaTeX{} source.  Execution order is from bottom to top, with the topmost node representing an output.  Index symbols have been converted to (lexically scoped) integers. Contractions are labeled with $*$ and a list of summation indices.  Named intermediate tensors are indicated with dummy, label nodes (= C[1,2]) etc.}
\label{f:dag}
\end{figure*}

  Next, the nodes of the DAG are transformed into local operations by replacing the index
labels with local permutation indices.  % using a top-down tree traversal.
The new Sum, Diff, and Dot nodes contain permutation lists for finding the input dimension
corresponding to a given output dimension.
Named intermediates are also linked into their parent at this stage.

  The result will be a DAG of tensor operations,
\begin{verbatim}
struct net_tensor {
  network_ptr *location;
     // where / how is the tensor accessed
  struct tensor *A; // actual data, if local
};

struct tens_gen {
  int n; // dimension
  int *shape; // list of n integers
  (*gen)(float *row, int *); // function generating row of values
};

struct tens_dot {
  struct tens_op *A, *B;
  int n; // number of output dimensions
  int m; // number of contracted dimensions
  int *dA, *dB; // list of n indices to output from A and B
  int *cA, *cB; // list of m indices to contract from A and B
};

struct tens_op {
  enum TensOP type; // 1 = tensor constant,
  				// 2 = tens_gen, 3 = tens_dot
  union {
    struct net_tensor *const; // type = 1
    struct tens_gen *gen; // type = 2
    struct tens_dot *dot; // type = 3
  };
};
\end{verbatim}

  This DAG will be the target of algebraic optimization techniques aimed at reducing the total operation
count.

  The overall operation tree will then be a {\tt struct tens\_op}.  When that top-level
{\tt tens\_op}'s type is constant, the computation is finished.  Otherwise, the chain of references
forms a directed acyclic graph with lots of pending operations (leaves of type tens\_dot).

  The reference execution model will find a {\tt tens\_op} with type {\tt tens\_dot} and two
leaves of type {\tt tens\_gen} or {\tt tens\_const}.  It will then execute the contraction, producing
a {\tt struct tensor} in local memory.  Then two leaves will be submitted to garbage collection (decrement
refcount), and the completed {\tt tens\_dot} will be replaced with the new value.

  Eventually, we may augment the tree with information to minimize communication.

\end{document}


\subsection{ Reference Tensor Representation}

  Drawing from the representation in the {\em numpy} library
(\href{http://docs.scipy.org/doc/numpy/reference/arrays.html}{ndarrays} -- see C-struct access),
a tensor is composed of a header and a large data block.  The header contains the following information about accessing elements of the data block

\begin{itemize}
\item n -- the logical dimension of the array
\item shape -- a list of n integers, stating the length along each dimension
\item strides -- a list of n integers, stating the number of bytes to skip for
travel along each direction
\end{itemize}

  For example, an order 4 tensor with shape (4,5,2,7) would have
$4*5*2*7 = 280$ floats.  The general element A[i,j,k,l] would accessed like:

\begin{verbatim}float *A;
int strides[] = {5*2*7, 2*7, 7, 1};
float x = A + i*strides[0] + j*strides[1] + k*strides[2] + l*strides[3];
\end{verbatim}

  Tensor transposition can be accomplished just by changing the header information.
For example, to access old element [i,j,k,l] like [i,l,k,j], we need to swap dimensions 1 and 3.
The strides then change from (5*2*7, 2*7, 7, 1) to (5*2*7, 1, 7, 2*7).  In {\em numpy}, this
is done with the call {\tt A = transpose(A, (0,3,2,1))}.

\subsection{ Reference Contraction}

  The basic operation is contraction.  Contraction of a (3,2,4) tensor $\tens{A}$
and a (2,4,7) tensor $\tens{B}$ along the (2,4) axes would produce a (3,7)
tensor like so:

\begin{verbatim}float *C = malloc(sizeof(float)*3*7);

for(k=0; k<3; k++) { for(l=0; l<7; l++) {

for(i=0; i<2; i++) {
   for(j=0; j<4; j++) {
     C[k,l] += A[k,i,j]*B[i,j,l];
   }
}

}}
\end{verbatim}

  Of course the memory accesses are indexed as above.  Contraction should always be
replaced by BLAS dgemm operations where possible.
These are likely already implemented in GPU libraries like Magma.

  Also, it makes sense to check for performance gains by completing several multiplications
at once.  This can be made to fit in GPU memory by breaking tensors into sub-blocks.
The theano library may provide a means of testing and and producing such optimizations.

\section{ Requirements}

  A library will be constructed containing the following core functionality:
\begin{enumerate}
  \item Tensor contraction (single-machine memory to memory
  \item DAG operations to select most suitable available contraction operation and manage storage of intermediate results (deleting when no longer needed as a dependency of another calculation).
  \item Interprocessor communication of a tensor. \\
   This will likely include setting up a pipeline for efficient transfer of larger data structures.  Overlapping communication and execution would be nice, but is not required at this stage.
  \item Tensor storage / read from disk \\
  This functionality is needed for storage of intermediate results in case the available memory of a machine is exhausted.
  Does mmap offer better performance for storing large tensors in memory?
\item Checking sizes of intermediate results. \\
  Eventually, this information will be used to better organize the order of operations.
\end{enumerate}

  The demo will invoke this library to solve the small-molecule test problem.

  Because the calculation is expected to be memory-bound, each compute node will operate by spawning
sub-processes (fork or thread) to execute ready-to-go operations from the bottom of the graph until its memory is exhausted.  One simple implementation strategy is to use mpi to launch one task per node and have that task spawn threads.

  Experience gained from this preliminary implementation will inform later
decisions on scheduling operations.

\section{ Possible FOSS Tools}

\begin{itemize}
  \item OpenMPI, high-level communication primitives (some RDMA support).  Note that GPU-direct RDMA is supported for sizes less than 30kb. \\
  \href{https://computing.llnl.gov/tutorials/mpi/samples/C/mpi\_latency.c}{latency test},
  \href{https://www.pdc.kth.se/education/tutorials/summer-school/mpi-exercises/mpi-lab-3-bandwidth-latency-and-timings}{timings}
  \item Magma, a GPU accelerated linear algebra library: \href{http://icl.cs.utk.edu/magma/}{http://icl.cs.utk.edu/magma/}
  \item Theano: a python tensor operation optimization library: \href{http://deeplearning.net/software/theano/index.html}{Docs} \\
   This has a backend for producing compiled CUDA code to do tensor contractions (float32 only)
  \item TCE the original tensor contraction engine: \href{http://www.csc.lsu.edu/\%7Egb/TCE/}{http://www.csc.lsu.edu/$\sim$gb/TCE/} \\
   As far as I can tell, this spreads each single tensor contraction over all nodes - a serious design limitation that was exacerbated by a team size that grew large and drifted in different directions.
That method of parallelization only works for the paradigm of having few contraction operations.
We have lots of contractions and need to spread those operations across nodes.   
  \item GNU Sci. Lib, a general purpose linear algebra library: \href{http://www.gnu.org/software/gsl/}{http://www.gnu.org/software/gsl/}
  \item \href{http://web.cs.uh.edu/~openuh/}{OpenUH} (supporting preliminary subset of OpenACC - \href{http://www.clustermonkey.net/Parallel-Programming/an-open-compiler-for-openacc.html}{announcement})
  \item Network coordination of tensor locations and remaining contractions can be managed through etcd or similar.  This will need a simple update mechanism for informing the network of accepted and complete jobs.
\end{itemize}

\section{ Demo Problem}

  The problem that will be solved for the initial publication of this method will be to reproduce the
implementation of the density-fitted CCSD equations as implemented by Eugene DePrince in
the GPU module (\href{https://github.com/edeprince3/gpu\_dfcc/blob/master/ccsd.cu}{gpu\_dfcc})
for \href{http://www.psicode.org/release_notes.php}{Psi4}.  The equations are listed
in \href{http://pubs.acs.org/doi/abs/10.1021/ct400250u}{DePrince and Sherrill, JCTC 9(6):2687, 2013.}
and more detail on the GPU implementation is given in
\href{}{DePrince, Kennedy, Sumpter, and Sherrill, Mol. Phys. 112: 844, 2014}.

  The DAG of primitive tensor operations can be read directly
from a subset of the \LaTeX{} math syntax and/or built by automated methods.
For example, the \LaTeX{} source for the first few elements of the residual equation:
\begin{align}
R_{ij}^{ab} &= \sum_Q M_{ai}^Q M_{bj}^Q + A_{ij}^{ab} + B_{ij}^{ab} + U_{ij}^{ab} + U_{ji}^{ba} \\
A_{ij}^{ab} &= \sum_{cd} t_{ij}^{cd} \sum_Q M_{ac}^Q M_{bd}^Q \\
B_{ij}^{ab} &= \sum_{kl} t_{kl}^{ab} \left( \sum_Q M_{ki}^Q M_{lj}^Q
                 + \sum_{cd} t_{ij}^{cd} \sum_Q M_{kc}^Q M_{ld}^Q \right)
\end{align}
was parsed into the expression tree in Fig.~\ref{f:dag}.  Here, $U_{ij}^{ab}$ is used to stand
for $\frac{1}{2}C_{ij}^{ab} + C_{ji}^{ab} + D_{ij}^{ab} + E_{ij}^{ab} + G_{ij}^{ab}$.

  It should be noted at the outset that the tensor names are non-unique without also giving
example indices.  This is because the tensor, $F \in \mathcal T_{o,v,o,v}$, for example,
can be written as the $ov \times ov$ matrix,
\begin{equation}
\begin{bmatrix}
\text{Fij} & \text{Fia} \\ \text{Fai} & \text{Fab}
\end{bmatrix}
.
\end{equation}
Most of the expressions in the paper related to one of the sub-blocks of a tensor
rather than the complete tensor.  This is acutely the case for $M$
($B_{\circ}^Q$ in the references).
The code again uses 4 different matrices,
\begin{equation}
\begin{bmatrix}
\text{Qoo}^Q & \text{Qov}^Q \\ \text{transpose}(\text{Qov})^Q =\text{Qvo}^Q & \text{Qvv}^Q
\end{bmatrix}
.
\end{equation}

\begin{figure*}
\includegraphics[width=0.9\textwidth]{dag.eps}
\caption{Initial expression tree parsed from \LaTeX{} source.  Execution order is from bottom to top, with the topmost node representing an output.  Index symbols have been converted to (lexically scoped) integers. Contractions are labeled with $*$ and a list of summation indices.  Named intermediate tensors are indicated with dummy, label nodes (= C[1,2]) etc.}
\label{f:dag}
\end{figure*}

  Next, the nodes of the DAG are transformed into local operations by replacing the index
labels with local permutation indices.  % using a top-down tree traversal.
The new Sum, Diff, and Dot nodes contain permutation lists for finding the input dimension
corresponding to a given output dimension.
Named intermediates are also linked into their parent at this stage.

  The hand-coded implementation of the residual calculation ($R_{ij}^{ab}$) is
done in {\tt pthreadCCResidual}:1066.  This routine is called by each of at least 2 pthreads.
Thread 0 executes the GPU portion, assigning all available GPUs to compute $A_{ij}^{ab}$, which has
the largest matrix multiplication to carry out.  Its task is broken into tiles that fit
into the GPU memory.  The sizes of the tensors are given by malloc statements on lines
991--1026.

  Other threads are used in a gang to compute the rest of the
contributions to the residual {\em via} {\tt \#pragma omp parallel for}.  As I understand
OMP scheduling, the {\tt parallel for} spawns its own threads, so only 1 thread should
really be executing the pragma (otherwise work is duplicated).  This computation
starts by assembling $\tfrac{1}{2}C_{ij}^{ab} + C_{ji}^{ab}$ into a temp space,
$\text{tempt}$, where
\begin{equation}
C_{ij}^{ab} = -\sum_{kc} t_{kj}^{bc} \left[
\sum_Q M_{ki}^Q M_{ac}^Q - \frac{1}{2} \sum_{ld} t_{li}^{ad} (\sum_Q M_{kd}^Q M_{lc}^Q)
\right]
\end{equation}
is computed from the starting matrices, $M = \text{Qov}$, and the doubles amplitudes,
$t_{ij}^{ab} = \text{tb}$, via the following steps (on lines 1114--1255):
\begin{align}
\text{Qov} &\in \mathcal T_{o,v,n_Q} \\
\text{Qoo} &\in \mathcal T_{o,o,n_Q} \\
\text{Qvv} &\in \mathcal T_{n_Q,v,v} \\
\text{integrals} &= \text{Qov} \cdot \text{Qov}^T  \in \mathcal T_{o,v,o,v} \\
\text{tempt}[aild] \in \mathcal T_{vovo} &= \text{tb}[adli] \in \mathcal T_{v,v,o,o} \\
\text{tempv}[kcld] \in \mathcal T_{ovov} &= \text{integrals}[kdlc] \in \mathcal T_{o,v,o,v} \\
\text{integrals} &= \frac{1}{2} \sum_{kc} \text{tempt}[aild] \text{tempv}[kcld] \quad \text{(tiled)} \\
\text{tempv} &= \text{Qvv}^T \cdot \text{Qoo}^T \\
\text{integrals} &= \text{integrals} + \text{tempv} \\
\text{tempt}[bjkc] &= \text{tb}[bckj] \\
\text{tempv}[bjai] &= -\sum_{kc} \text{integrals}[acki] \text{tempt}[bckj] \quad \text{(tiled)} \\
\text{tempt}[abij] &= \frac{1}{2} \text{tempv}[bjai] + \text{tempv}[biaj]
.
\end{align}
This symmetrized sum of $C$ is then added to the residual and stored on disk.
The calculation then takes a break and does the singles residual,
\begin{equation}
R_i^a = F_{ai} + \sum_{dQ} \sum_{kc} u_{ki}^{cd} M_{kc}^Q M_{ad}^Q
  - \sum_{klc} u_{kl}^{ac} \sum_Q M_{ki}^Q M_{lc}^Q
  + \sum_{kc} F_{kc} u_{ik}^{ac}
  ,
\end{equation}
from which $u_{ij}^{ab}$ (in {\tt tempt}) is a by-product (which is recomputed later anyway?).

  The doubles calculation picks back up on line 1360, with $D$, $E$, $G$, and $B$.
The $\sum_Q M^Q_{kd}M^Q_{lc}$ part is shared between $E$ and $G$ using the
`integrals' buffer.
Each time it reads the last residual from the disk,
adds the new contribution, and stores the result.

  Finally $A$ is added into the residual ({\tt useVabcd1}) after all threads complete and control returns to
{\tt CCResidual:1704}.  If the cpu calculations finish first, {\tt FinishVabcd1} aids the gpu in completing the
calculation of $A$.  It is unclear what part of the code calls {\tt T1Fock} to compute $F_{ia}$ and $F_{ai}$,
but a comment notes that it is recomputed at each iteration -- so it's likely the DIIS driver that also calls {\tt CCResidual}.

  Many of these calculations also use matrix tiling, illustrated by Fig.~2 of the Mol. Phys. article.
In particular, the tiling of the $A$ tensor will need to be considered to minimize idle time.
Basically, the contraction is divided into smaller sub-blocks of the output.  The picture is like Strassen's matrix
multiplication algorithm, where the output is divided into $2\times 2$ blocks, $\begin{bmatrix}a_{00} & a_{01} \\ a_{10} & a_{11} \end{bmatrix}$.
\begin{align}
A &\gets B \cdot C \\
& \to A \gets \text{combine}(B[:n/2] \cdot C[:,:m/2], B[:n/2] \cdot C[:,m/2:], \ldots)
\end{align}
This can be put into the operation DAG by introducing {\tt combine} and {\tt split} operations.
A contraction over a tensor with large dimension sizes can be replaced by a series of
contractions sandwiched between these combine and split operations.

  The $A$ tensor also has symmetry which allows it to be stored as 2
compressed, lower-diagonal matrices.
One is symmetric and the other anti-symmetric with respect to
interchanging either $i\leftrightarrow j$ or $a\leftrightarrow b$
(equations 31-37 of the JCTC paper).

  The memory is accessed using the {\tt Position} function,
called wherever tensor symmetry is used.
See especially the lines 254--310 where $A$ is added to the
$\mathcal T_{v,v,o,o}$ residual (exactly the shape of the {\tt t2}
matrix, confusingly called {\tt tb} in the code).
Symmetry will require replacing the Dot operation in the task DAG
with a symmetry-aware contraction operation, and adding
a special flag to the $A$ tensor to note its symmetric storage type.

\end{document}


  Although the transpositions scale only as $O(N^2)$, while the contraction
scales formally as $O(N^3)$, both operations are severely limited by memory access times.
The theoretical maximum host to device memory bandwidth (using PCI-E 3.0 x16) is 16 GB/s.
Dividing by the three matrices and 8 bytes per float, this limits single-precision matrix multiplication
to a maximum output speed of 0.67 Gfloats/s.
%  On-device memory bandwidth is 10x higher, on the order of 100 GB/s.
However, there are $N^3$ floating point operations
per matrix multiply, so the GEMM speed (Gflops/s) will be calculated by multiplying the
output rate by $3 N$.  The peak floating performance of 2000~Gflops/s will only
be achieved after N=1000.
For smaller problem sizes, matrix multiplication and transposition
are both limited by data transfer rates, and will therefore have similar scaling behavior.

  For the usual case, emulating tensor contractions by the transpose-matrix multiply-transpose strategy
is more than a factor of 3 slower than performing the contraction directly.  In the usual case, most codes
implement the transpose using simple loops.  This can cause another factor of 10 slow-down because
their memory access pattern is much worse than the matrix multiplication.

  Two strategies will be pursued to remove this problem to create a direct contraction.  First,
we will create a code generator to produce kernels for tensor contraction.  Each run of the kernel
will produce an $nc$-dimensional sub-block of the output tensor.  The size of the nested inner loops
within the kernel should be as close as possible to the processor's local cache.
We will base these kernels on the general strategy used by similar kernels within
the Magma\cite{magma} library (Fig.~\ref{f:mm}).
A group of threads divide the work of computing an output tile in two different ways.
First, they pre-fetch large $N\times K$ and $M\times K$ sub-blocks of the two input matrices
from slower device memory into a faster region shared by all threads.  Next, they each
compute a different sub-tile of the $N\times M$ output tile.  Finally, each thread outputs
its own sub-tile.  There are several copying steps in-between to explicitly use thread-local
memory registers.

  In a general tensor contraction, the pre-fetches will be multidimensional blocks
from the two input matrices.  These will still be stored in 2-dimensional arrays
on the device, mirroring the transpose-multiply-transpose strategy.  The difference
here is that the transpose steps are grouped with the matrix multiplication.
Thus, the memory need only be moved once instead of 3 times, resulting in at least
a factor of 3 improvement.

  The second strategy will be to slightly modify existing routines for fast matrix multiplication
to allow outputting to matrices with 2 strides, instead of the usual single stride (i.e. `ldc').
This will allow the overall operation will work by picking 2 dimensions to be output matrix-at-a-time
while looping over all other $nc-2$.
Matrix multiplication corresponds to a subset of the kernels that could be generated by the first approach.
Namely, matrix multiplication corresponds to output sub-blocks of the form $(1,1,N,1,M)$, or $(N,M,1,1)$, etc.,
where only two of the $nc$ output dimensions are not 1.  Since matrix multiplication routines
derive most of their speed by grouping reads that are stored sequentially in memory,
the two output strides will be determined based on where the smallest dimension of the two
input matrices is going.  Four cases are possible.  First, both input matrices may have
their smallest (sequential) dimension uncontracted -- going to an output.  This corresponds
to a series of $A B^T$ (Fortran ordering).  Second, input may be contracted and the other
uncontracted.  This corresponds to a series of $A^T B^T$ or $AB$.  Cases three and four
occur when both smaller indices are contracted.  If the indices are contracted against one another,
any outer indices can be chosen to write this as a series of $A^T B$ operations (case three).
However, the fourth case does not map directly to a matrix operation.
It corresponds most closely to a series of direct matrix products
followed by summation, which may pay a huge penalty in storage space.

  Both strategies leverage existing fast kernels for matrix multiplication.  The second
may be expected to give quick results and perform well when rank of the tensor's smallest
dimension is large.  However, the code generation strategy covers cases that the second
strategy misses, including multiple contracted dimensions and small last-dimension sizes.
It also offers more opportunity for tuning and improvement.

